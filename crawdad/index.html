<!doctype html><html lang=en><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Using crawdad</title><meta name=description content><meta itemprop=name content="Using crawdad"><meta itemprop=description content><meta name=twitter:card content=summary><meta name=twitter:title content="Using crawdad"><meta name=twitter:description content><meta name=twitter:site content=@yakczar><meta name=twitter:creator content=@yakczar><meta name=og:title content="Using crawdad"><meta name=og:description content><meta name=og:type content=website><link rel=stylesheet href=/css/style2.css><link rel=stylesheet href=/fonts/1406-fonts.css><script src=/js/caption.js></script><style></style><img src="https://analytics.schollz.com/1.png?page=https%3a%2f%2fschollz.github.io%2fcrawdad%2f" width=1px height=1px style=float:right><header><small><strong><a href=https://schollz.github.io style=color:#000>/posts</a>&nbsp;&nbsp;·&nbsp;
<a href=https://schollz.github.io/about/ style=color:#000>/about</a>&nbsp;&nbsp;·&nbsp;
<a href=https://schollz.github.io/contact/ style=color:#000>/contact</a></strong></small></header><div class=content><h1>Using crawdad</h1><p><img src=https://user-images.githubusercontent.com/6550035/31456157-58663efe-ae76-11e7-8e53-6a2a5b7a196c.png alt="A new, simple, powerful content extractor"><p><em>crawdad</em> is a simple, yet powerful alternative for scraping in a distributed, persistent manner (backed by Redis). It can do simple things, like generating site maps. It can also do complicated things, like extracting all the quotes from every page on a quotes website (tutorial follows).<h2 id=install>Install</h2><p>First <a href=https://www.docker.com/community-edition>get Docker</a> which will be used for running Redis.<p>Then you can simply download <em>crawdad</em>:<pre><code class=language-sh>$ wget https://github.com/schollz/crawdad/releases/download/v3.0.0/crawdad_3.0.0_linux_amd64.zip
$ unzip crawdad*zip
$ sudo mv crawdad*amd64 /usr/local/bin/crawdad
</code></pre><p>Unlike many other scraping frameworks, <em>crawdad</em> is a single binary that has no dependencies.<h2 id=configure>Configure</h2><p>For scraping, <em>crawdad</em> requires creating a <a href=https://github.com/schollz/pluck#use-config-file>pluck configuration file</a>. Here is the configuration file for scraping <a href=http://quotes.toscrape.com>quotes.toscrape.com</a>:</p><script src=https://gist.github.com/schollz/02205b5c1a3c5ade132e17ce61ce1213.js></script><p><em>pluck</em> is a language-agnostic way of extracting structured data from text without HTML/CSS/Regex. Essentially <em>pluck</em> is configured in a way you would tell your friend to grab data.<p>For example, the first <em>pluck</em> unit describes how you would get the quote text from <a href=http://quotes.toscrape.com>quotes.toscrape.com</a>. Starting from the beginning of the source, you look for the string &ldquo;<code>span class=&quot;text&quot;</code>&rdquo; (called an <em>activator</em>). Once that is found, you look for a &ldquo;<code>&gt;</code>&rdquo;, the next activator. Then you capture all the characters until a &ldquo;<code>&lt;</code>&rdquo; is seen (the <em>deactivator</em>). This will allow you to collect all the quotes.<p>Each of the <em>pluck</em> units will be found simultaneously and extracted from any HTML page crawled by <em>crawdad</em>.<h2 id=run>Run</h2><p>First, start Redis with Docker:<pre><code class=language-sh>$ docker run -d -p 6374:6379 redis
</code></pre><p>and then start <em>crawdad</em>:<pre><code>$ time crawdad -p 6374 -set -u http://quotes.toscrape.com -pluck quotes.toml -include '/page/' -exclude '/tag/'
0.12s user 0.03s system 5% cpu 2.666 total
</code></pre><p>The <code>-set</code> flag tells the <em>crawdad</em> to create some new settings with a URL (<code>-u</code>) and a <em>pluck</em> configuration (<code>-pluck</code>) and with some inclusions/exclusions (<code>-include</code>/<code>-exclude</code>). The inclusions and exclusions ensures that only the <code>/page</code> links will be followed (in order to compare with scrapy).<h2 id=extract-data>Extract data</h2><p>The data from <em>crawdad</em> can be parsed in the same as <em>scrapy</em> by first dumping it,<pre><code>$ crawdad -p 6374 -done done.json
</code></pre><p>The data, <code>done.json</code>, contains each URL as a key and the data it extracted. It needs to be quickly parsed, too, which can be done lickity-split in Python in 12 lines of code:</p><script src=https://gist.github.com/schollz/f27547bb4716fc14fd574e9bbdad57a1.js></script><h2 id=crawdad-bonuses><em>crawdad</em> bonuses</h2><p><em>crawdad</em> has some other mighty benefits as well. Once initiated, you can run another crawdad on a different machine:<pre><code class=language-sh>$ crawdad -p 6374 -s X.X.X.X
</code></pre><p>This will start crawling using the same parameters as the first <em>crawdad</em>, but will pull from the queue. Thus, you can easily make a distributed crawler.<p>Also, since it is backed by Redis, <em>crawdad</em> is resilient to interruptions and allows you to restart from the point that it was interrupted. Try it!<h1 id=comparison-with-scrapy>Comparison with <em>scrapy</em></h1><p>Here I will compare scraping the same site, <a href=http://quotes.toscrape.com/>quotes.toscrape.com</a> with <em>crawdad</em> (my creation) and <em>scrapy</em> (the popular framework for scraping).<p><img src=https://user-images.githubusercontent.com/6550035/31486741-b06865e4-aef5-11e7-8b0d-c5ed107b25b4.png alt="scrapy is really useful tool to get started in scraping."><p><em>scrapy</em> is powerful, but complicated. Lets follow the tutorial to get a baseline on how a scrapy should run.<h2 id=install-1>Install</h2><p>First install <em>scrapy</em> by installing the dependencies (there are a lot of dependencies).<pre><code class=language-sh>$ sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
$ sudo -H python3 -m pip install --upgrade scrapy
</code></pre><p>Once you get it install you can check the version:<pre><code class=language-sh>$ scrapy -version
Scrapy 1.4.0 - project: quotesbot
</code></pre><h2 id=configure-1>Configure</h2><p>Actually, I will just use the <a href=https://github.com/scrapy/quotesbot>tutorial of <em>scrapy</em></a> to skip building it myself.<pre><code class=language-sh>$ git clone https://github.com/scrapy/quotesbot.git
$ cd quotesbot
</code></pre><p><em>scrapy</em> is not simple. It requires &gt; 40 lines of Python code in several different files (<code>items.py</code>, <code>pipelines.py</code>, <code>settings.py</code>, <code>spiders/toscrape-css.py</code>).<h2 id=run-1>Run</h2><p>Lets run and time the result:<pre><code>$ time scrapy crawl toscrape-xpath -o quotes.json
1.06s user 0.08s system 29% cpu 3.877 total
</code></pre><p><em>scrapy</em> is about 10-30% slower than <em>crawdad</em>, plus it can not easily be run in a distributed, persistent way.</div><footer><p><small><em>Written October 11, 2017. Read more <a href=https://schollz.github.io/tags/coding/ class=w3-tag>coding</a>.</em></small><p><small>Leave a comment for me on Twitter <a href="https://twitter.com/intent/tweet?text=%40yakcar%20%23crawdad%20">@yakczar</a>.</small><p><a href=/docker-news/>← Read news in the terminal with Docker</a>&nbsp;<a href=/words/ style=float:right>What we say has never been said →</a></span></footer>